"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8363],{3905:function(e,t,a){a.d(t,{Zo:function(){return d},kt:function(){return p}});var n=a(67294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var c=n.createContext({}),l=function(e){var t=n.useContext(c),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},d=function(e){var t=l(e.components);return n.createElement(c.Provider,{value:t},e.children)},h={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,o=e.originalType,c=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),u=l(a),p=i,m=u["".concat(c,".").concat(p)]||u[p]||h[p]||o;return a?n.createElement(m,r(r({ref:t},d),{},{components:a})):n.createElement(m,r({ref:t},d))}));function p(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=a.length,r=new Array(o);r[0]=u;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s.mdxType="string"==typeof e?e:i,r[1]=s;for(var l=2;l<o;l++)r[l]=a[l];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},84399:function(e,t,a){a.r(t),a.d(t,{frontMatter:function(){return s},contentTitle:function(){return c},metadata:function(){return l},toc:function(){return d},default:function(){return u}});var n=a(87462),i=a(63366),o=(a(67294),a(3905)),r=["components"],s={id:"cachingdesigns",title:"E-Samwad Caching Designs",sidebar_label:"E-Samwad Caching Designs",sidebar_position:6},c=void 0,l={unversionedId:"E-Samwad/cachingdesigns",id:"E-Samwad/cachingdesigns",isDocsHomePage:!1,title:"E-Samwad Caching Designs",description:"1. Caching",source:"@site/docs/E-Samwad/E-Samwad-Caching-Designs.md",sourceDirName:"E-Samwad",slug:"/E-Samwad/cachingdesigns",permalink:"/docs/docs/E-Samwad/cachingdesigns",editUrl:"https://github.com/Samarth-HP/docs/docs/E-Samwad/E-Samwad-Caching-Designs.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{id:"cachingdesigns",title:"E-Samwad Caching Designs",sidebar_label:"E-Samwad Caching Designs",sidebar_position:6},sidebar:"tutorialSidebar",previous:{title:"Deploy Samwad",permalink:"/docs/docs/E-Samwad/deploysamwad"},next:{title:"Docusaurus Introduction",permalink:"/docs/docs/mermaid"}},d=[{value:"1. Caching",id:"1-caching",children:[]},{value:"2. Analysis",id:"2-analysis",children:[]},{value:"3. Design for current caching implementation in eSamwad:",id:"3-design-for-current-caching-implementation-in-esamwad",children:[]},{value:"4. Improvements",id:"4-improvements",children:[]},{value:"5. Ongoing",id:"5-ongoing",children:[]}],h={toc:d};function u(e){var t=e.components,s=(0,i.Z)(e,r);return(0,o.kt)("wrapper",(0,n.Z)({},h,s,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h2",{id:"1-caching"},"1. Caching"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"caching_flow",src:a(93030).Z})),(0,o.kt)("p",null,"Caching Design proved to be one of the most important aspect in esamwad's applcation layer to maintain application performance, high availibility and high tolerance. It helped in our scaling decisions too.\nWhen pondering around the use cases of esamwad around the schools in Himachal Pradesh and amount of load it will be creating, different designs for caching was considered."),(0,o.kt)("h2",{id:"2-analysis"},"2. Analysis"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"We knew that recently requested data is likely to be requested again by many different users, so by taking advantage of the locality of reference principle we leveraged this to reduce our API response time and improve applications performance.")),(0,o.kt)("p",null,(0,o.kt)("img",{src:a(7301).Z})),(0,o.kt)("ol",{start:2},(0,o.kt)("li",{parentName:"ol"},"Then, it was the decision on which layer to apply caching on. Ex- client, web server, application or on database.\nWhile there were pros and cons for every decision that could have been taken. The important part was to decide what will be best suited for our use case, i.e. to serve APIs which return repititive data and are stable on high load.")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Though we do have a MongoDB Realm SDK used on our Android App used for caching and offline support, but it has dependencies on phone memory and third party permissions. So client side caching was not the optimal solution.")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"mongodb-realm",src:a(38382).Z})),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"No doubt, server side caching would have proved great for application like ours which needs high throughput and performance but they became challenging when run on a government restricted server where third party reverse proxies and web accelerators like Amazon CloudFront, CloudFlare, etc are mostly restricted and their pricing and maintainance becomes issue after sometimes. Also ready to use proxies like memcached, etc have been tried previously on esamwad older versions but I guess it would also become a hassle to ship so many components in a single deployment package and separately maintain each of them. This may prove good enough in a private organisations but our goal is to make our services with high standards and low maintainance supports.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Application sided caching never suited in our use case as it keeps a cache directly on the Application server. Each time a request is made to the service, the node will quickly return local, cached data if it exists. If not, the requesting node will query the data by going to network storage such as a database. When the application server is expanded to many nodes, we may haved faced the following issues"),(0,o.kt)("ol",{parentName:"li"},(0,o.kt)("li",{parentName:"ol"},"The load balancer randomly distributing requests across the nodes."),(0,o.kt)("li",{parentName:"ol"},"The same request can go to different nodes, increase cache misses."),(0,o.kt)("li",{parentName:"ol"},"Extra storage since the same data will be stored in two or more different\nnodes.\nSolutions for the issues would have been:"),(0,o.kt)("li",{parentName:"ol"},"Global caches"),(0,o.kt)("li",{parentName:"ol"},"Distributed caches"))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Database caching was also one of the optimal options as it helps in resolving our problem for maintaining global caches. And one easy implementation we found for leveraging it was using Hasura Event Triggers."))),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"hasura_event_triggers",src:a(18003).Z})),(0,o.kt)("p",null,"Hasura can be used to create event triggers on tables in the Postgres database. Event triggers reliably capture events on specified tables and invoke webhooks to carry out any custom logic. Events can be anything INSERT, UPDATE, DELETE, MANUAL."),(0,o.kt)("p",null,"In previous application version, we used event triggers on our major tables whose events affected the change in our APIs, after which the cache entries need to be updated and this was done through a webhook API. This seems to be a good fashioned way of maintaining and revalidating cache in a serverless fashion without much dependencies on the main application. While it is promising, but it may cause issues when the events counts increase as it will cross swords with original applications database storage and will fill up event meta data table very frequently which then again needs to be cleared from time to time."),(0,o.kt)("p",null,"To tackle this issue we decided to leverage django signals for updating our cache tables on event signals without adding extra dependencies on clearing the from time to time. "),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"django_signals",src:a(24778).Z})),(0,o.kt)("p",null,"Django includes a \u201csignal dispatcher\u201d which helps decoupled applications get notified when actions occur elsewhere in the framework. In a nutshell, signals allow certain senders to notify a set of receivers that some action has taken place. They\u2019re especially useful when many pieces of code may be interested in the same events.\nDjango provides a set of built-in signals that let user code get notified by Django itself of certain actions."),(0,o.kt)("h2",{id:"3-design-for-current-caching-implementation-in-esamwad"},"3. Design for current caching implementation in eSamwad:"),(0,o.kt)("p",null,"We decided on this design for caching in our current application version to store our cache globally in our main database and have added a feature flag ",(0,o.kt)("em",{parentName:"p"},"IS_CACHE_ENABLED")," to control the caching in the application in a more modular way without hindering with overall application code structure."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"caching-esamwad",src:a(90392).Z})),(0,o.kt)("p",null,"We started our implementation around the signals and developed logics to revalidate our cache on each signal triggers by sending tasks through celery to a queue manager which would asyncronously update the cache tables. Though it seemed to work properly but these also had their own challenges."),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"The workers on queue manager (RabbitMQ in our case) runs the task asynchronous so when a task is sent to the queue manager there is no callback for the  responses of the task, i.e if it failed or completed successfully. So in cases if there is too much lag on the queue manager it would take uncertain amount of time to determine when the task will get completed or even if it got completed or not. "),(0,o.kt)("li",{parentName:"ol"},"Other issue was the performance of the application, as signals themselves are synchronous in nature so it affected the API performance causing a little overtime on APIs response time when a signal event was triggered. It caused lag in the admin console too, on updating the entries in table through admin console it didn't ensure if the queue manager will update the cache in realtime and user may still seem to see older data for a certain time. The major challenge here that we faced was having a good performance on the broker side so that tasks sent could be executed in realtime and should be fast. One of the ways to improve the performance that we figured out was  internal query optimisations in API logic layer which in turn made the API calls very slow and workers performance seem to be affected with this causing the little lags in the tasks getting executed."),(0,o.kt)("li",{parentName:"ol"},"To ensure no duplicate tasks are sent to the broker or to ensure if other parallel task on the broker does not change the previous queued tasks we added the queue lock logics and flags to ensure no duplicate task gets added meanwhile one task is getting executed, but it too started causing issues and race conditions on the broker which resulted in lags and application crashes.")),(0,o.kt)("h2",{id:"4-improvements"},"4. Improvements"),(0,o.kt)("p",null,"Facing challenges on improving the API performance and broker issues, we decided to first start from optimising our internal queries and bussiness logics."),(0,o.kt)("p",null,"We did manage to optimise our API and internal database calls to achieve the maximum performance in least time and were able to bring down the average response time of 6.0-8.0 secs on our main meta API to an average of 1.5-2.0 secs. This significantly improved the benchamrk results on the API side."),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"Benchmarks results before optimisation:")),(0,o.kt)("p",null,(0,o.kt)("img",{src:a(89065).Z})),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"Benchmarks results before optimisation:")),(0,o.kt)("p",null,(0,o.kt)("img",{src:a(30410).Z})),(0,o.kt)("p",null,"But still it was not the end of the road down here, the problem with broker performance and availibility was still casuing the issues."),(0,o.kt)("p",null,"While exploring more solutions on how to reduce the api response time, we decided to encrypt the data while storing and decrypt it while fetching from the database which would result in high throughput and low response time. One of the standards for enryption we decided to test was ",(0,o.kt)("a",{parentName:"p",href:"https://pypi.org/project/zstandard/"},"zstandard"),".\nWe had a POC on encrypting the cache data before storing it in database and then decrypting in on demand when API calls are made. The compression was around 60x, and it in turn reduced the API response time significantly. Now the average API response time that we observed on staging environments were 300-400 ms for a non cached data retrieval and around 50-150 ms for cached data retrieval."),(0,o.kt)("p",null,(0,o.kt)("img",{src:a(22101).Z})),(0,o.kt)("h2",{id:"5-ongoing"},"5. Ongoing"),(0,o.kt)("p",null,"So we achived the best performance for our APIs from this, now the task remained to increase the performance and availibility of the broker and properly leverage the django signals so that no tasks are skipped and the caches are updated automatically on signal calls."),(0,o.kt)("p",null,"We also have periodic celery beat tasks which is again configured through an environment variable which after specified amount of time goes and revalidate the caches even if no triggers occur to ensure that caches are always updated."),(0,o.kt)("p",null,"For this we are creating a list of use cases for each signals from the django-dispatcher and how it should be interpreted to update the cache."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"}," CURRENT STATUS")," : WIP"))}u.isMDXComponent=!0},38382:function(e,t,a){t.Z=a.p+"assets/images/MongoDB-Realm-1155cc3ed0dd6d5056b61318ec42b520.png"},22101:function(e,t,a){t.Z=a.p+"assets/images/after_compression-b6befad98f8e683ea15b65173e77cc5b.png"},30410:function(e,t,a){t.Z=a.p+"assets/images/benchmarks_after_optimisation-7748f14479aaacb97289ce85a0c00f0b.PNG"},89065:function(e,t,a){t.Z=a.p+"assets/images/benchmarks_before_optimisation-e826169cd2c8b977cdca2a1bab3ee326.PNG"},90392:function(e,t,a){t.Z=a.p+"assets/images/caching-esamwad-959d207161b19a2aa9f9c36c916c05ca.PNG"},93030:function(e,t,a){t.Z=a.p+"assets/images/caching-layer-dd04dcf2bf14f1eb903977e0310c405e.png"},7301:function(e,t,a){t.Z=a.p+"assets/images/caching-635732290e7882c27826f35b12cfaae7.png"},18003:function(e,t,a){t.Z=a.p+"assets/images/hasura-events-3ce0c4446307756d6cf30d492fd44eaf.png"},24778:function(e,t,a){t.Z=a.p+"assets/images/signals-django-8d528ee5536a325df3a3c2b1b628f467.png"}}]);