"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[627],{3905:(e,t,n)=>{n.d(t,{Zo:()=>m,kt:()=>g});var r=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,i=function(e,t){if(null==e)return{};var n,r,i={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=r.createContext({}),c=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},m=function(e){var t=c(e.components);return r.createElement(l.Provider,{value:t},e.children)},p="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),p=c(n),d=i,g=p["".concat(l,".").concat(d)]||p[d]||u[d]||o;return n?r.createElement(g,a(a({ref:t},m),{},{components:n})):r.createElement(g,a({ref:t},m))}));function g(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,a=new Array(o);a[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[p]="string"==typeof e?e:i,a[1]=s;for(var c=2;c<o;c++)a[c]=n[c];return r.createElement.apply(null,a)}return r.createElement.apply(null,n)}d.displayName="MDXCreateElement"},1010:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var r=n(7462),i=(n(7294),n(3905));const o={title:"Samarth - Monitoring",sidebar_position:7,sidebar_label:"Monitoring"},a=void 0,s={unversionedId:"tech/monitoring",id:"tech/monitoring",title:"Samarth - Monitoring",description:"Need",source:"@site/docs/tech/monitoring.md",sourceDirName:"tech",slug:"/tech/monitoring",permalink:"/docs/tech/monitoring",draft:!1,editUrl:"https://github.com/Samarth-HP/docs/tree/master/docs/tech/monitoring.md",tags:[],version:"current",sidebarPosition:7,frontMatter:{title:"Samarth - Monitoring",sidebar_position:7,sidebar_label:"Monitoring"},sidebar:"overview",previous:{title:"Analytics",permalink:"/docs/tech/analytics"},next:{title:"Crash Reporting",permalink:"/docs/tech/crash-reporting"}},l={},c=[{value:"Need",id:"need",level:2},{value:"Implementation",id:"implementation",level:2},{value:"Monitoring",id:"monitoring",level:3},{value:"Host Monitoring",id:"host-monitoring",level:4},{value:"Services Monitoring",id:"services-monitoring",level:4},{value:"Load Monitoring",id:"load-monitoring",level:4},{value:"Alerting",id:"alerting",level:2},{value:"Benefits",id:"benefits",level:2}],m={toc:c},p="wrapper";function u(e){let{components:t,...o}=e;return(0,i.kt)(p,(0,r.Z)({},m,o,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h2",{id:"need"},"Need"),(0,i.kt)("p",null,"Monitoring a deployed system in production is an extremely important step."),(0,i.kt)("p",null,"Lack of monitoring leads to the below issues:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Deployed services & servers become black boxes"),(0,i.kt)("li",{parentName:"ul"},"Systems can go down unexpectedly"),(0,i.kt)("li",{parentName:"ul"},"Wait for users to inform about failures"),(0,i.kt)("li",{parentName:"ul"},"Engineers / PMs are escalated to fix issues reactively"),(0,i.kt)("li",{parentName:"ul"},"Constant firefighting"),(0,i.kt)("li",{parentName:"ul"},"Pattern identification is super tough")),(0,i.kt)("p",null,"With monitoring to following advantages are seen:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Deployed systems & servers are monitored actively & automatically"),(0,i.kt)("li",{parentName:"ul"},"Systems going down trigger alerts"),(0,i.kt)("li",{parentName:"ul"},"System health degradation can help proactively fix issues"),(0,i.kt)("li",{parentName:"ul"},"With a stabler proactively fixed system we can actively work on features"),(0,i.kt)("li",{parentName:"ul"},"Patterns are identified quickly")),(0,i.kt)("p",null,"Our top goals to setup monitoring for the Samarth systems are:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Identify performance of backend systems"),(0,i.kt)("li",{parentName:"ul"},"Setup alerting when a system is degrading or offline")),(0,i.kt)("h2",{id:"implementation"},"Implementation"),(0,i.kt)("p",null,"Monitoring & alerting for Samarth system is powered by a Grafana + Prometheus system. Grafana and\nprometheus are industry standards when it comes to production deployment monitoring and alerting. We\nchose them as they are open source and can be self hosted, which both are important requirements."),(0,i.kt)("h3",{id:"monitoring"},"Monitoring"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Monitoring Working",src:n(871).Z,width:"993",height:"433"}),"\n",(0,i.kt)("a",{parentName:"p",href:"https://www.ansible.com/blog/red-hat-ansible-tower-monitoring-using-prometheus-node-exporter-grafana"},"Image Source")),(0,i.kt)("h4",{id:"host-monitoring"},"Host Monitoring"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Host Monitoring",src:n(4963).Z,width:"1194",height:"577"})),(0,i.kt)("p",null,"Here we monitor the server processes, load average, uptime, storage, etc.."),(0,i.kt)("h4",{id:"services-monitoring"},"Services Monitoring"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Services Monitoring",src:n(6955).Z,width:"1194",height:"570"})),(0,i.kt)("p",null,"Here we monitor the service response times, uptimes and load."),(0,i.kt)("h4",{id:"load-monitoring"},"Load Monitoring"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Load Monitoring",src:n(8661).Z,width:"1195",height:"580"})),(0,i.kt)("p",null,"Here we monitor the load on the host. We use it extensively to see which service / container is\nconsuming how much memory. It helps in identifying unnatural peaks in traffic."),(0,i.kt)("h2",{id:"alerting"},"Alerting"),(0,i.kt)("p",null,"We have setup alerting to let us know when:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"A system is above 80% load"),(0,i.kt)("li",{parentName:"ul"},"A system is low on memory"),(0,i.kt)("li",{parentName:"ul"},"A service goes down")),(0,i.kt)("p",null,"These alerts are linked to our communication channels where devs can proactively check and fix\nissues rather than having to monitor the grafana dashboard constantly."),(0,i.kt)("h2",{id:"benefits"},"Benefits"),(0,i.kt)("p",null,"Monitoring has helped us identify some excessive load issues and proactively fix issues before the\nsystem goes down completely. It has also helped us identify large response times from services which\nwe can fix proactively."))}u.isMDXComponent=!0},4963:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/host_monitoring-33f60108a6ef2240037d02da78fbf3d4.png"},8661:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/load_monitoring-df9632718e8170a25a112ce34555b281.png"},871:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/prometheus_grafana-fe2b16a6e5025d2440f6435851a3579e.webp"},6955:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/services_monitoring-1265d539084918466e5451934c0867f4.png"}}]);